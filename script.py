import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import roc_auc_score
from sklearn import preprocessing
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from scipy import sparse
from scipy.sparse import vstack, hstack, csr_matrix
from sklearn.model_selection import train_test_split

data = pd.read_csv("../input/malware-prediction-am/train.csv")
data['HasDetections'].value_counts()# target

# # Preprocessing for train data

stats = []
for col in data.columns:
    stats.append((col,data[col].nunique(), data[col].isnull().sum(), data[col].isnull().sum() * 100 / data.shape[0], data[col].dtype))
    
stats_df = pd.DataFrame(stats, columns=['Feature','Unique_values','null count', 'Percentage of missing values', 'type'])

stats_df.sort_values('Percentage of missing values', ascending=False).head(10)


for col in data.columns:
  
    if (data[col].isnull().sum()>0):
        if data[col].dtype==object:
            data[col].fillna(data[col].mode()[0], inplace = True)
        
        else:

            if data[col].dtype==int:
                data[col].fillna(data[col].mode()[0], inplace = True)
        
            else :
                data[col].fillna(data[col].mean(), inplace = True)
# For features with data type as integer and object the missing values are filled with mode and those with data type as float its missing values are filled with the mean of the data.


cc=[]
for col in data.columns:
    if data[col].dtype!=object:
        cc.append(col)

sk=[] #features that are skewed
sk=['Census_OSBuildRevision','Census_InternalPrimaryDiagonalDisplaySizeInInches','Census_SystemVolumeTotalCapacity','Census_PrimaryDiskTotalCapacity','Census_TotalPhysicalRAM']

dpp=[] # features that are to be dropped
dpp=['PuaMode', 'Census_ProcessorClass', 'DefaultBrowsersIdentifier', 'Census_IsFlightingInternal', 'Census_InternalBatteryType',
'Census_ThresholdOptIn', 'Census_IsWIMBootEnabled','MachineIdentifier']

ytrain=data['HasDetections']  #target

faldata=data
faldata.drop(faldata[dpp],axis=1,inplace=True) #dropping unwanted features

faldata.drop('HasDetections',axis=1,inplace=True)

for c in sk:
    faldata[c]=np.log(faldata[c]+1) #removing skew

ob=[]# list of features with data type "objects"
for col in faldata.columns:
    if faldata[col].dtype==object:
        ob.append(col)


onehot=preprocessing.OneHotEncoder(handle_unknown='ignore')
col_1hot=onehot.fit_transform(faldata[ob])# one hot encoding of the coulums of data type "object"
#this will return a sparse matrix


faldata.drop(faldata[ob],axis=1,inplace=True)


trainmat=sparse.csr_matrix(faldata) # transforming non object type colums to matrix form

Xtrain=sparse.hstack([trainmat,col_1hot]).tocsr()  # combining the matix of object and non object coulums

###for test data
testdata = pd.read_csv("../input/malware-prediction-am/test.csv")
tmm=testdata['MachineIdentifier']

testfinal=testdata
testfinal.drop(testfinal[dpp],axis=1,inplace=True) #dropping unnessasry features

# # Preprocessing test data
for col in testfinal.columns:
    if (testfinal[col].isnull().sum()>0):
        if testfinal[col].dtype==object:
            testfinal[col].fillna(testfinal[col].mode()[0], inplace = True)
        else:
            if testfinal[col].dtype==int:
                testfinal[col].fillna(testfinal[col].mode()[0], inplace = True)
            else :
                testfinal[col].fillna(testfinal[col].mean(), inplace = True)


for c in sk:
    testfinal[c]=np.log(testfinal[c]+1)

test1=onehot.transform(testfinal[ob]) # onehot encoding for test

testfinal.drop(testfinal[ob],axis=1,inplace=True)
testmat=sparse.csr_matrix(testfinal)

Xtest=sparse.hstack([testmat,test1]).tocsr() #our final xtest

# # Regression

#declaring hyper parameters for lgbm
reg=LGBMClassifier(objective='binary',n_estimators=1000,max_depth=10,num_leaves=512,colspample_bytree=0.3,learning_rate=0.05)

reg.fit(Xtrain,ytrain,eval_set=[(Xtrain,ytrain)],eval_metric='auc',early_stopping_rounds=100)
#fitting the data to lgbm. here we have to choose early_stopping_rounds low compared to n estimaters if a high value is chosen it starts to overfit

lgbm_Ytest=reg.predict_proba(Xtest)[:,1] # predicting the Malware detection for test using lgbm.
testfinal['lgbm']=lgbm_Ytest

testfinal['MachineIdentifier']=tmm

#declaring hyper parameters for xgboost
xgb = XGBClassifier(objective='binary:logistic',n_estimators=1000,max_depth=7,num_leaves=75,colspample_bytree=0.3,learning_rate=0.12)
xgb.fit(Xtrain,ytrain) #fitting the train data to xgboost model
xgb_Ytest=xgb.predict_proba(Xtest)[:,1]  # predicting the Malware detection for test using Xgboost model

testfinal['xgb']=xgb_Ytest
testfinal['HasDetections']=(0.435*testfinal['xgb'])+ (0.565*testfinal['lgbm']) #ensembling
# these specific weights are obtained from the below process these weights are the best for our data.

testfinal[['MachineIdentifier','HasDetections']].to_csv('output.csv', index=False) #finally combining the MachineIdentifier and target HasDetections to give a csv output.

# chechking our output
out=pd.read_csv("./output.csv")
out.describe()


## Finding weights for ensembling

X0_train, X0_test, y0_train, y0_test = train_test_split(Xtrain,ytrain,test_size=0.33, random_state=42)
#splitting the train data randomly to find the best weights for ensembling


reg.fit(X0_train,y0_train,eval_set=[(X0_train,y0_train)],eval_metric='auc',early_stopping_rounds=100) #fitting lgbm with the same hyper parameters used for the test data of the competetion.
Y11=reg.predict_proba(X0_test)[:,1]

xgb.fit(X0_train,y0_train) #fitting xgboost with the same hyper parameters used for the test data of the competetion.
sp=xgb.predict_proba(X0_test)[:,1]

# below loops are to find the first second and third decimal for best combination of weights
# higher the auc score implies better accuracy for given parameters and the used hyper parameters we got weights as 0.565,0.435 for lgbm and xgboost

#output of this loop we will be the best possible weight combination with precession upto one decimal  
for i in range (1,10):
    tp=(i*Y11)+((10-i)*sp)
    tpp=tp/10
    print(i)
    print(roc_auc_score(y0_test,tpp))

#output of this loop we will be the best possible weight combination with precession upto second decimal  
for i in range (1,20):
    tp=(i*Y11)+((20-i)*sp)
    tpp=tp/20
    print(i/20)
    print(roc_auc_score(y0_test,tpp))

#from the previous loop we have seen that the best score is between 0.5 and 0.6 so we ran another loop which gives best possible weight combination and precession upto third decimal
for j in range (1,20):
    q=j/200
    i=0.5+q
    k=0.5-q
    tp=(i*Y11)+((k)*sp)
    tpp=tp/20
    print(i)
    print(roc_auc_score(y0_test,tpp))

# the obatained best weights for the given trianing data set and hyperparameters for the models is 0.565 for lgbm and 0.435 for xgboost
